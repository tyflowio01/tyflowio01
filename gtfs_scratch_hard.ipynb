{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3I/XTc+nM423+6btr6q7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyflowio01/tyflowio01/blob/main/gtfs_scratch_hard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GTFS Data and IoT Integration Solution\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates the integration of GTFS (General Transit Feed Specification) data and IoT (Internet of Things) data using Google Cloud Platform (GCP) services. The solution processes GTFS data and real-time data from IoT devices, stores it in BigQuery, and visualizes it using a Streamlit app.\n",
        "\n",
        "## Function of this Notebook\n",
        "\n",
        "1. **GCP Authentication and Project Selection**: Authenticate and select the GCP project to use for this solution.\n",
        "2. **Install Required Libraries**: Install necessary Python libraries for working with GCP services and Streamlit.\n",
        "3. **Enable Required APIs**: Enable the required Google Cloud APIs for this solution.\n",
        "4. **Select GCS Bucket**: Select a Google Cloud Storage bucket to store the GTFS data.\n",
        "5. **Download GTFS Data**: Download GTFS data from a specified source or upload a local file.\n",
        "6. **Create Pub/Sub Topic**: Create a Pub/Sub topic for IoT data.\n",
        "7. **Load GTFS Schema into BigQuery**: Infer the schema from the downloaded GTFS files and create corresponding tables in BigQuery.\n",
        "8. **Install and Set Up Streamlit**: Install Streamlit and set up a Streamlit app for data visualization.\n",
        "9. **Dataflow Pipeline**: Configure and run a Dataflow pipeline to process data from Pub/Sub and write it to BigQuery.\n",
        "10. **Visualization**: Use the Streamlit app to visualize the real-time data stored in BigQuery.\n"
      ],
      "metadata": {
        "id": "po44Wg2GczX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block Diagram of the GTFS Data and IoT Integration Solution\n",
        "\n",
        "```plaintext\n",
        " +-------------+      +-----------+      +-------------+\n",
        " |             |      |           |      |             |\n",
        " |  GTFS Data  |      |  IoT      |      |  Storage    |\n",
        " |  Source     |----->|  Device   |----->|  Bucket     |\n",
        " |             |      |  (Moxa)   |      |             |\n",
        " +-------------+      +-----------+      +-------------+\n",
        "        |                  |                   |\n",
        "        v                  v                   |\n",
        " +-------------+      +-----------+            |\n",
        " |             |      |           |            |\n",
        " |  Simulate   |      |  MQTT     |            |\n",
        " |  Data       |      |  Broker   |            |\n",
        " +-------------+      +-----------+            |\n",
        "        |                  |                   |\n",
        "        v                  v                   v\n",
        " +--------------------------------------------+\n",
        " |                                            |\n",
        " |                Pub/Sub Topic               |\n",
        " |                (iot-data-topic)            |\n",
        " +--------------------------------------------+\n",
        "                            |\n",
        "                            v\n",
        "                     +-------------+\n",
        "                     |             |\n",
        "                     |  Dataflow   |\n",
        "                     |  Pipeline   |\n",
        "                     +-------------+\n",
        "                            |\n",
        "                            v\n",
        "                     +-------------+\n",
        "                     |             |\n",
        "                     |  BigQuery   |\n",
        "                     |  Table      |\n",
        "                     |  (gtfs_data)|\n",
        "                     +-------------+\n",
        "                            |\n",
        "                            v\n",
        "                     +-------------+\n",
        "                     |             |\n",
        "                     |  Streamlit  |\n",
        "                     |  App        |\n",
        "                     +-------------+\n",
        "                            |\n",
        "                            v\n",
        "                    Visualization\n"
      ],
      "metadata": {
        "id": "RZo90TkjiUPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Flow\n",
        "- **GTFS Data Source**: Obtain GTFS data.\n",
        "- **Upload to GCS Bucket**: Store the GTFS data in a Google Cloud Storage bucket.\n",
        "- **Pub/Sub Topic (iot-data)**: Publish the data from IoT devices to this topic.\n",
        "- **Dataflow Pipeline (gtfs-dataflow)**: Process the data from Pub/Sub and write it to BigQuery.\n",
        "- **BigQuery Table (gtfs_data)**: Store the processed data in a BigQuery table.\n",
        "- **Streamlit App**: Visualize the data.\n",
        "- **Visualization**: Display the real-time data.\n"
      ],
      "metadata": {
        "id": "IowbKwNyb9EX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pBMary-BPqm"
      },
      "outputs": [],
      "source": [
        "# Cell 0: GCP Authentication and Project Selection\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Authenticate the user\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get the list of projects\n",
        "cloud_resource_manager = build('cloudresourcemanager', 'v1')\n",
        "projects = cloud_resource_manager.projects().list().execute()\n",
        "\n",
        "# Display the list of projects\n",
        "print(\"Select a project from the list below:\")\n",
        "project_list = [project['projectId'] for project in projects.get('projects', [])]\n",
        "for idx, project in enumerate(project_list):\n",
        "    print(f\"{idx + 1}. {project}\")\n",
        "\n",
        "# Prompt user to select a project\n",
        "project_index = int(input(\"Enter the number of the project you want to use: \")) - 1\n",
        "project_id = project_list[project_index]\n",
        "print(f\"Selected project: {project_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Required Libraries\n",
        "!pip install apache-beam[gcp] streamlit pyngrok google-cloud-storage google-cloud-pubsub google-cloud-bigquery\n"
      ],
      "metadata": {
        "id": "l7obFUjxBalZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: GCP Authentication and Project Selection\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Authenticate the user\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get the list of projects\n",
        "cloud_resource_manager is build('cloudresourcemanager', 'v1')\n",
        "projects = cloud_resource_manager.projects().list().execute()\n",
        "\n",
        "# Display the list of projects\n",
        "print(\"Select a project from the list below:\")\n",
        "project_list = [project['projectId'] for project in projects.get('projects', [])]\n",
        "for idx, project in enumerate(project_list):\n",
        "    print(f\"{idx + 1}. {project}\")\n",
        "\n",
        "# Prompt user to select a project\n",
        "project_index = int(input(\"Enter the number of the project you want to use: \")) - 1\n",
        "project_id = project_list[project_index]\n",
        "print(f\"Selected project: {project_id}\")\n"
      ],
      "metadata": {
        "id": "WUf0GCw7Be5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enable Required APIs\n",
        "!gcloud config set project {project_id}\n",
        "!gcloud services enable storage.googleapis.com pubsub.googleapis.com cloudfunctions.googleapis.com bigquery.googleapis.com run.googleapis.com dataflow.googleapis.com\n"
      ],
      "metadata": {
        "id": "QqUvurdoBgTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Select GCS Bucket\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client with the selected project\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "# List available buckets\n",
        "buckets = list(storage_client.list_buckets())\n",
        "print(\"Select a bucket from the list below:\")\n",
        "bucket_list = [bucket.name for bucket in buckets]\n",
        "for idx, bucket_name in enumerate(bucket_list):\n",
        "    print(f\"{idx + 1}. {bucket_name}\")\n",
        "\n",
        "# Prompt user to select a bucket\n",
        "bucket_index = int(input(\"Enter the number of the bucket you want to use: \")) - 1\n",
        "bucket_name = bucket_list[bucket_index]\n",
        "print(f\"Selected bucket: {bucket_name}\")\n"
      ],
      "metadata": {
        "id": "Zyriush9Gkll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Download GTFS Data\n",
        "import requests\n",
        "from google.cloud import storage\n",
        "from google.colab import files\n",
        "\n",
        "# Define GTFS data source URLs for San Francisco Bay Area\n",
        "gtfs_sources = [\n",
        "    \"https://transitfeeds.com/p/sfmta/60/latest/download\",  # BART\n",
        "    \"http://transitfeeds.com/p/ac-transit/406/latest/download\",  # AC Transit\n",
        "    \"https://data.trilliumtransit.com/gtfs/marin-transit-ca-us/marin-transit-ca-us.zip\"  # Marin Transit\n",
        "]\n",
        "\n",
        "# Provide options to the user\n",
        "print(\"Select a GTFS data source:\")\n",
        "print(\"1. BART (San Francisco Bay Area Rapid Transit)\")\n",
        "print(\"2. AC Transit\")\n",
        "print(\"3. Marin Transit\")\n",
        "print(\"4. Upload from local file\")\n",
        "source_option = int(input(\"Enter the number of your choice: \"))\n",
        "\n",
        "# Download GTFS data from the selected source\n",
        "if source_option in [1, 2, 3]:\n",
        "    gtfs_url = gtfs_sources[source_option - 1]\n",
        "    response = requests.get(gtfs_url)\n",
        "    gtfs_file_path = 'gtfs_data.zip'\n",
        "    with open(gtfs_file_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "elif source_option == 4:\n",
        "    print(\"Please upload your GTFS zip file.\")\n",
        "    uploaded = files.upload()\n",
        "    gtfs_file_path = list(uploaded.keys())[0]\n",
        "else:\n",
        "    print(\"Invalid selection. Please run the cell again and select a valid option.\")\n",
        "    gtfs_file_path = None\n",
        "\n",
        "if gtfs_file_path:\n",
        "    # Upload GTFS data to GCS bucket\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob('gtfs_data.zip')\n",
        "    blob.upload_from_filename(gtfs_file_path)\n",
        "    print(f\"Uploaded GTFS data to: gs://{bucket_name}/gtfs_data.zip\")\n",
        "else:\n",
        "    print(\"No GTFS data uploaded.\")\n"
      ],
      "metadata": {
        "id": "8kpB5JnJGl5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create Pub/Sub Topic\n",
        "import subprocess\n",
        "\n",
        "# Define the Pub/Sub topic name\n",
        "topic_name = 'iot-data-topic'\n",
        "\n",
        "# Create the Pub/Sub topic\n",
        "subprocess.run([\n",
        "    'gcloud', 'pubsub', 'topics', 'create', topic_name,\n",
        "    '--project', project_id\n",
        "])\n",
        "print(f\"Created Pub/Sub topic: {topic_name}\")\n"
      ],
      "metadata": {
        "id": "5O4oMAwIBz_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Load GTFS Schema into BigQuery from Downloaded Files\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Function to infer schema from GTFS files\n",
        "def infer_schema_from_gtfs(gtfs_file_path):\n",
        "    schemas = {}\n",
        "\n",
        "    with zipfile.ZipFile(gtfs_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('gtfs_data')\n",
        "\n",
        "    for filename in os.listdir('gtfs_data'):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join('gtfs_data', filename)\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                schema = []\n",
        "                for column in df.columns:\n",
        "                    dtype = df[column].dtype\n",
        "                    if dtype == 'int64':\n",
        "                        field_type = 'INTEGER'\n",
        "                    elif dtype == 'float64':\n",
        "                        field_type = 'FLOAT'\n",
        "                    elif dtype == 'bool':\n",
        "                        field_type = 'BOOLEAN'\n",
        "                    elif 'datetime' in str(dtype):\n",
        "                        field_type = 'TIMESTAMP'\n",
        "                    else:\n",
        "                        field_type = 'STRING'\n",
        "                    schema.append(bigquery.SchemaField(column, field_type))\n",
        "                schemas[filename] = schema\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {e}\")\n",
        "    return schemas\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bigquery_client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Define the dataset name\n",
        "dataset_id = f\"{project_id}.gtfs_dataset\"\n",
        "\n",
        "# Create the dataset if it does not exist\n",
        "try:\n",
        "    bigquery_client.get_dataset(dataset_id)  # Make an API request.\n",
        "    print(f\"Dataset {dataset_id} already exists.\")\n",
        "except Exception:\n",
        "    dataset = bigquery.Dataset(dataset_id)\n",
        "    dataset.location = \"US\"\n",
        "    bigquery_client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "    print(f\"Created dataset {dataset_id}.\")\n",
        "\n",
        "# Infer schema from GTFS files\n",
        "schemas = infer_schema_from_gtfs(gtfs_file_path)\n",
        "\n",
        "# Create tables in BigQuery based on inferred schema\n",
        "for filename, schema in schemas.items():\n",
        "    table_name = filename.replace('.txt', '')\n",
        "    table_id = f\"{dataset_id}.{table_name}\"\n",
        "\n",
        "    try:\n",
        "        bigquery_client.get_table(table_id)  # Make an API request.\n",
        "        print(f\"Table {table_id} already exists.\")\n",
        "    except Exception:\n",
        "        table = bigquery.Table(table_id, schema=schema)\n",
        "        bigquery_client.create_table(table)  # Make an API request.\n",
        "        print(f\"Created table {table_id} with schema inferred from {filename}.\")\n",
        "\n",
        "print(\"Schema inference and table creation completed.\")\n"
      ],
      "metadata": {
        "id": "UleAjplraoXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create BigQuery Dataset\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bigquery_client is bigquery.Client(project=project_id)\n",
        "\n",
        "# Define the dataset name\n",
        "dataset_id = f\"{project_id}.gtfs_dataset\"\n",
        "\n",
        "# Create the dataset if it does not exist\n",
        "try:\n",
        "    bigquery_client.get_dataset(dataset_id)  # Make an API request.\n",
        "    print(f\"Dataset {dataset_id} already exists.\")\n",
        "except Exception:\n",
        "    dataset = bigquery.Dataset(dataset_id)\n",
        "    dataset.location = \"US\"\n",
        "    bigquery_client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "    print(f\"Created dataset {dataset_id}.\")\n"
      ],
      "metadata": {
        "id": "YU-kD3JiB5_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Install Streamlit and Set Up for Colab\n",
        "!pip install streamlit pyngrok\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Prompt the user for their ngrok authtoken\n",
        "ngrok_authtoken = input(\"Please enter your ngrok authtoken: \")\n",
        "ngrok.set_auth_token(ngrok_authtoken)\n",
        "\n",
        "# Define the Streamlit app content\n",
        "streamlit_app_code = f\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def run_streamlit():\n",
        "    st.title('GTFS Data Visualization')\n",
        "\n",
        "    client = bigquery.Client(project='{project_id}')\n",
        "    query = \\\"\"\"\n",
        "    SELECT id, timestamp, latitude, longitude\n",
        "    FROM `{project_id}.gtfs_dataset.gtfs_data`\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 100\n",
        "    \\\"\"\"\n",
        "    query_job = client.query(query)\n",
        "    results = query_job.result()\n",
        "    df = results.to_dataframe()\n",
        "\n",
        "    st.write(\"Latest GTFS Data:\")\n",
        "    st.dataframe(df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_streamlit()\n",
        "\"\"\"\n",
        "\n",
        "# Write the Streamlit app code to a file\n",
        "with open('streamlit_app.py', 'w') as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"streamlit_app.py\"])\n",
        "\n",
        "# Wait a few seconds for the Streamlit app to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(addr='8501', proto='http')\n",
        "print(f\"Streamlit public URL: {public_url}\")\n",
        "\n",
        "# Print the public URL\n",
        "print(f\"Streamlit app is running at: {public_url}\")\n"
      ],
      "metadata": {
        "id": "qstl5RNvSUjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Dataflow Pipeline\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
        "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
        "from apache_beam.io.gcp.pubsub import ReadFromPubSub\n",
        "import json\n",
        "\n",
        "# Define the pipeline options\n",
        "options = PipelineOptions(\n",
        "    streaming=True,\n",
        "    runner='DataflowRunner',\n",
        "    project=project_id,\n",
        "    region='us-central1',\n",
        "    staging_location=f'gs://{bucket_name}/dataflow/staging',\n",
        "    temp_location=f'gs://{bucket_name}/dataflow/temp',\n",
        "    job_name='gtfs-iot-dataflow-pipeline'\n",
        ")\n",
        "\n",
        "# Define the pipeline\n",
        "def run():\n",
        "    with beam.Pipeline(options=options) as p:\n",
        "        (p\n",
        "         | 'ReadFromPubSub' >> ReadFromPubSub(topic=f'projects/{project_id}/topics/iot-data-topic')\n",
        "         | 'ParseJson' >> beam.Map(lambda msg: json.loads(msg.decode('utf-8')))\n",
        "         | 'WriteToBigQuery' >> WriteToBigQuery(\n",
        "             table=f'{project_id}:gtfs_dataset.gtfs_data',\n",
        "             schema='id:STRING, timestamp:TIMESTAMP, sensor_value:FLOAT',\n",
        "             create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
        "         )\n",
        "        )\n",
        "\n",
        "# Start the pipeline\n",
        "import threading\n",
        "\n",
        "def start_pipeline():\n",
        "    run()\n",
        "\n",
        "# Run the pipeline in a separate thread\n",
        "pipeline_thread = threading.Thread(target=start_pipeline)\n",
        "pipeline_thread.start()\n",
        "\n",
        "print(\"Dataflow pipeline has been started in the background.\")\n"
      ],
      "metadata": {
        "id": "3rZLIlWCCO5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional"
      ],
      "metadata": {
        "id": "WX3N6v8LEgH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10-iot: Integrate IoT Device (Optional)\n",
        "# This cell is optional and should be used to integrate a new IoT device into the Dataflow pipeline.\n",
        "# It sets up the IoT device to publish data to the Pub/Sub topic.\n",
        "\n",
        "from google.cloud import pubsub_v1\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "MOXA_IP = '192.168.1.100'  # IP address of the Moxa ioLogik\n",
        "MOXA_PORT = 502  # Modbus/TCP port\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID'\n",
        "TOPIC_ID = 'iot-data-topic'\n",
        "\n",
        "# Initialize Pub/Sub publisher\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "topic_path = publisher.topic_path(PROJECT_ID, TOPIC_ID)\n",
        "\n",
        "# Function to read data from Moxa device\n",
        "def read_moxa_data(client):\n",
        "    # Replace this with the actual Modbus addresses and data format for your Moxa device\n",
        "    rr = client.read_holding_registers(0, 10, unit=1)\n",
        "    return rr.registers\n",
        "\n",
        "# Function to publish messages\n",
        "def publish_message(data):\n",
        "    message_data = json.dumps(data).encode('utf-8')\n",
        "    future = publisher.publish(topic_path, message_data)\n",
        "    future.result()  # Ensure the message is published\n",
        "\n",
        "def main():\n",
        "    # Connect to the Moxa device\n",
        "    client = ModbusTcpClient(MOXA_IP, port=MOXA_PORT)\n",
        "    client.connect()\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            # Read data from Moxa device\n",
        "            data = read_moxa_data(client)\n",
        "            # Publish data to Pub/Sub\n",
        "            publish_message(data)\n",
        "            # Wait for a defined interval\n",
        "            time.sleep(5)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    finally:\n",
        "        client.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hT9wo4XjCSpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator"
      ],
      "metadata": {
        "id": "orMPt8xpEnhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Data Generator for Sending Messages through the Pipeline\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from google.cloud import pubsub_v1\n",
        "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
        "\n",
        "# Function to generate random GTFS data\n",
        "def generate_random_gtfs_data():\n",
        "    data = {\n",
        "        'id': random.randint(1000, 9999),\n",
        "        'timestamp': int(time.time()),\n",
        "        'latitude': round(random.uniform(-90, 90), 6),\n",
        "        'longitude': round(random.uniform(-180, 180), 6)\n",
        "    }\n",
        "    return json.dumps(data)\n",
        "\n",
        "# Function to publish messages to Pub/Sub\n",
        "def publish_messages(project_id, topic_name, message_count, interval):\n",
        "    publisher = pubsub_v1.PublisherClient()\n",
        "    topic_path = publisher.topic_path(project_id, topic_name)\n",
        "\n",
        "    for _ in range(message_count):\n",
        "        message = generate_random_gtfs_data()\n",
        "        try:\n",
        "            future = publisher.publish(topic_path, message.encode('utf-8'))\n",
        "            future.result()  # Ensure the message is published\n",
        "            print(f\"Published message: {message}\")\n",
        "        except (GoogleAPICallError, RetryError) as e:\n",
        "            print(f\"An error occurred while publishing the message: {e}\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "# Prompt the user for the project_id\n",
        "project_id = input(\"Enter your GCP project ID: \")\n",
        "\n",
        "# Parameters\n",
        "topic_name = 'iot-data-topic'  # Ensure this matches the Pub/Sub topic name\n",
        "message_count = 1000  # Total number of messages to send for testing\n",
        "interval = 1  # Interval in seconds between messages\n",
        "\n",
        "# Start publishing messages\n",
        "publish_messages(project_id, topic_name, message_count, interval)\n"
      ],
      "metadata": {
        "id": "8tM2E0qNOSJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Security Analysis\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This section provides a security analysis of the GTFS Data and IoT Integration Solution, highlighting key security considerations and best practices to ensure the integrity, confidentiality, and availability of the system.\n",
        "\n",
        "### Authentication and Authorization\n",
        "\n",
        "1. **GCP Authentication**: The solution uses Google Cloud's built-in authentication mechanisms to ensure only authorized users can access the GCP resources. Users authenticate via OAuth2, which provides secure access tokens.\n",
        "   \n",
        "2. **IAM Roles and Permissions**: Proper Identity and Access Management (IAM) roles are assigned to users and service accounts. This ensures that only authorized entities can perform specific actions on GCP resources such as Pub/Sub, BigQuery, and Cloud Storage. Fine-grained IAM roles should be used to follow the principle of least privilege.\n",
        "\n",
        "### Data Security\n",
        "\n",
        "1. **Data Encryption**:\n",
        "   - **At Rest**: Data stored in Google Cloud Storage and BigQuery is encrypted at rest using Google-managed encryption keys by default.\n",
        "   - **In Transit**: Data transferred between clients and GCP services is encrypted using TLS (Transport Layer Security) to protect against interception and eavesdropping.\n",
        "\n",
        "2. **Secure Storage Access**: Access to the Google Cloud Storage bucket is controlled using IAM policies to ensure only authorized users and service accounts can read or write data.\n",
        "\n",
        "### Network Security\n",
        "\n",
        "1. **Firewall Rules**: Implement firewall rules to restrict access to the VMs and other GCP resources to only trusted IP addresses. This helps prevent unauthorized access from external networks.\n",
        "\n",
        "2. **Private Networking**: Utilize VPC (Virtual Private Cloud) to create isolated networks for different parts of the solution. Private networking ensures that internal traffic between GCP services does not traverse the public internet, reducing the attack surface.\n",
        "\n",
        "### Logging and Monitoring\n",
        "\n",
        "1. **Audit Logging**: Enable Cloud Audit Logs to track administrative activities and access to GCP resources. Audit logs help in detecting unauthorized access attempts and monitoring the usage of critical resources.\n",
        "\n",
        "2. **Monitoring and Alerts**: Use Google Cloud Monitoring and Alerts to keep track of the system's health and performance. Set up alerts for unusual activities or performance degradation to respond promptly to potential security incidents.\n",
        "\n",
        "### Data Integrity\n",
        "\n",
        "1. **Data Validation**: Implement data validation checks when ingesting data from IoT devices and GTFS sources to ensure the data's integrity and accuracy.\n",
        "\n",
        "2. **Error Handling**: Incorporate robust error handling mechanisms in the data processing pipeline to gracefully handle and log errors without compromising data integrity.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "By adhering to these security best practices, the GTFS Data and IoT Integration Solution ensures a secure and reliable environment for processing and visualizing transit data. Regular security assessments and updates to the security policies will help maintain the solution's integrity and protect against emerging threats.\n"
      ],
      "metadata": {
        "id": "F6APDLWldQNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Analysis\n",
        "\n",
        "This section provides a cost analysis of the GTFS Data and IoT Integration Solution, based on one million iterations or invocations. The analysis includes the cost of using Google Cloud Storage, Pub/Sub, Dataflow, BigQuery, and Streamlit.\n",
        "\n",
        "### Google Cloud Storage\n",
        "\n",
        "- **Storage Cost**: \\$0.026 per GB per month\n",
        "- **Data Retrieval Cost**: \\$0.01 per GB\n",
        "\n",
        "Assuming 10 GB of storage and 1 GB of data retrieval:\n",
        "- Storage: 10 GB * \\$0.026 = \\$0.26\n",
        "- Data Retrieval: 1 GB * \\$0.01 = \\$0.01\n",
        "\n",
        "Total Cloud Storage Cost: **\\$0.27**\n",
        "\n",
        "### Google Cloud Pub/Sub\n",
        "\n",
        "- **Message Ingestion**: \\$0.40 per million messages\n",
        "- **Message Delivery**: \\$0.25 per million messages\n",
        "\n",
        "For 1 million messages:\n",
        "- Ingestion: 1 million * \\$0.40 = \\$0.40\n",
        "- Delivery: 1 million * \\$0.25 = \\$0.25\n",
        "\n",
        "Total Pub/Sub Cost: **\\$0.65**\n",
        "\n",
        "### Google Cloud Dataflow\n",
        "\n",
        "- **Dataflow Cost**: \\$0.01 per vCPU hour and \\$0.01 per GB hour\n",
        "\n",
        "Assuming a simple job running for 1 hour using 4 vCPUs and processing 10 GB of data:\n",
        "- vCPU Hours: 4 vCPUs * 1 hour * \\$0.01 = \\$0.04\n",
        "- GB Hours: 10 GB * 1 hour * \\$0.01 = \\$0.10\n",
        "\n",
        "Total Dataflow Cost: **\\$0.14**\n",
        "\n",
        "### Google Cloud BigQuery\n",
        "\n",
        "- **Storage Cost**: \\$0.02 per GB per month\n",
        "- **Query Cost**: \\$5.00 per TB processed\n",
        "\n",
        "Assuming 10 GB of storage and 1 TB of query data:\n",
        "- Storage: 10 GB * \\$0.02 = \\$0.20\n",
        "- Query: 1 TB * \\$5.00 = \\$5.00\n",
        "\n",
        "Total BigQuery Cost: **\\$5.20**\n",
        "\n",
        "### Streamlit\n",
        "\n",
        "- **Streamlit Sharing**: Free for limited use\n",
        "\n",
        "Assuming no additional costs for Streamlit.\n",
        "\n",
        "### Total Cost Analysis\n",
        "\n",
        "- **Google Cloud Storage**: \\$0.27\n",
        "- **Google Cloud Pub/Sub**: \\$0.65\n",
        "- **Google Cloud Dataflow**: \\$0.14\n",
        "- **Google Cloud BigQuery**: \\$5.20\n",
        "- **Streamlit**: Free\n",
        "\n",
        "Total Estimated Cost for One Million Iterations: **\\$6.26**\n",
        "\n",
        "### Sources\n",
        "\n",
        "- [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n",
        "- [Google Cloud Pub/Sub Pricing](https://cloud.google.com/pubsub/pricing)\n",
        "- [Google Cloud Dataflow Pricing](https://cloud.google.com/dataflow/pricing)\n",
        "- [Google Cloud BigQuery Pricing](https://cloud.google.com/bigquery/pricing)\n",
        "\n",
        "This cost analysis is based on estimates and actual costs may vary depending on usage patterns and other factors.\n"
      ],
      "metadata": {
        "id": "ntJ-WAt4fy3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanitization"
      ],
      "metadata": {
        "id": "Rk-JZaeJEYnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up GCP environment script\n",
        "\n",
        "from google.cloud import storage, bigquery, pubsub_v1\n",
        "from googleapiclient.discovery import build\n",
        "import subprocess\n",
        "\n",
        "def list_buckets():\n",
        "    storage_client = storage.Client()\n",
        "    buckets = list(storage_client.list_buckets())\n",
        "    return buckets\n",
        "\n",
        "def stop_dataflow_jobs(project_id):\n",
        "    dataflow = build('dataflow', 'v1b3')\n",
        "    jobs = dataflow.projects().locations().jobs().list(\n",
        "        projectId=project_id, location='us-central1').execute()\n",
        "\n",
        "    if 'jobs' in jobs:\n",
        "        for job in jobs['jobs']:\n",
        "            if job['currentState'] == 'JOB_STATE_RUNNING':\n",
        "                job_id = job['id']\n",
        "                dataflow.projects().locations().jobs().update(\n",
        "                    projectId=project_id,\n",
        "                    location='us-central1',\n",
        "                    jobId=job_id,\n",
        "                    body={'requestedState': 'JOB_STATE_CANCELLED'}\n",
        "                ).execute()\n",
        "                print(f\"Cancelled Dataflow job: {job_id}\")\n",
        "    else:\n",
        "        print(\"No running Dataflow jobs found.\")\n",
        "\n",
        "def delete_bigquery_table(project_id):\n",
        "    bigquery_client = bigquery.Client(project=project_id)\n",
        "    dataset_id = f\"{project_id}.gtfs_dataset\"\n",
        "    table_id = f\"{dataset_id}.gtfs_data\"\n",
        "\n",
        "    try:\n",
        "        bigquery_client.delete_table(table_id)\n",
        "        print(f\"Deleted BigQuery table: {table_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting BigQuery table: {e}\")\n",
        "\n",
        "def delete_files_in_bucket(bucket_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    blobs = list(bucket.list_blobs())\n",
        "    for blob in blobs:\n",
        "        blob.delete()\n",
        "        print(f\"Deleted file: {blob.name}\")\n",
        "\n",
        "    print(f\"All files deleted in bucket: {bucket_name}\")\n",
        "\n",
        "def delete_cloud_functions(project_id):\n",
        "    functions = build('cloudfunctions', 'v1')\n",
        "    result = functions.projects().locations().functions().list(\n",
        "        parent=f\"projects/{project_id}/locations/-\").execute()\n",
        "\n",
        "    if 'functions' in result:\n",
        "        for function in result['functions']:\n",
        "            function_name = function['name']\n",
        "            functions.projects().locations().functions().delete(\n",
        "                name=function_name).execute()\n",
        "            print(f\"Deleted Cloud Function: {function_name}\")\n",
        "    else:\n",
        "        print(\"No Cloud Functions found.\")\n",
        "\n",
        "def delete_pubsub_resources(project_id, topic_name):\n",
        "    publisher = pubsub_v1.PublisherClient()\n",
        "    subscriber = pubsub_v1.SubscriberClient()\n",
        "    topic_path = publisher.topic_path(project_id, topic_name)\n",
        "\n",
        "    subscriptions = list(subscriber.list_subscriptions(request={\"project\": f\"projects/{project_id}\"}))\n",
        "    for subscription in subscriptions:\n",
        "        if subscription.topic == topic_path:\n",
        "            subscriber.delete_subscription(request={\"subscription\": subscription.name})\n",
        "            print(f\"Deleted subscription: {subscription.name}\")\n",
        "\n",
        "    publisher.delete_topic(request={\"topic\": topic_path})\n",
        "    print(f\"Deleted Pub/Sub topic: {topic_path}\")\n",
        "\n",
        "def main():\n",
        "    project_id = input(\"Enter your GCP project ID: \")\n",
        "\n",
        "    stop_dataflow_jobs(project_id)\n",
        "    delete_bigquery_table(project_id)\n",
        "\n",
        "    # List buckets and prompt user to select one\n",
        "    buckets = list_buckets()\n",
        "    print(\"Select a bucket to delete data from:\")\n",
        "    for idx, bucket in enumerate(buckets):\n",
        "        print(f\"{idx + 1}. {bucket.name}\")\n",
        "\n",
        "    bucket_index = int(input(\"Enter the number of the bucket you want to delete data from: \")) - 1\n",
        "    bucket_name = buckets[bucket_index].name\n",
        "\n",
        "    delete_files_in_bucket(bucket_name)\n",
        "    delete_cloud_functions(project_id)\n",
        "    delete_pubsub_resources(project_id, 'iot-data-topic')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "faH0XagZEWew"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}